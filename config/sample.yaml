# Model settings
model:
  model_id: "Qwen/Qwen2.5-0.5B-Instruct"
  trust_remote_code: true
  max_num_seqs: 1
  enable_lora: false # Set to true to enable LoRA adapters

# LoRA settings (optional)
# Set only if you want to use a specific LoRA adapter
# Note: If you enable LoRA, there is a known prompt-logprob bug (https://discuss.vllm.ai/t/bug-wrong-lora-mapping-during-prompt-logprobs-computing/500/2).
# lora:
#   lora_name: "adapter_name"
#   lora_int_id: 1
#   lora_path: "adapter_path"

# sampling_parameters settings
sampling_parameters:
  max_tokens: 1
  prompt_logprobs: 0
  temperature: 0.0
  top_p: 1.0

# Data settings
data:
  # Basic settings
  data_path: "./data/sample_data.csv"  # Path to data file or huggingface dataset name
  format: "csv"                        # Data format (csv, jsonl, json, parquet, huggingface)

  # Column name settings
  text_column: "input"               # Name of text column
  label_column: "label"              # Name of label column

  # text split settings
  text_length: 32                    # Number of words to split (for WikiMIA dataset: 32, 64, 128, or 256)

  # Language
  space_delimited_language: true

# Evaluation methods
methods:
  - type: "loss"
    params: {}
  - type: "lower"
    params: {}
  - type: "zlib"
    params: {}
  - type: "mink"
    params:
      ratio: 0.1

# Output settings
output_dir: "./results" 
