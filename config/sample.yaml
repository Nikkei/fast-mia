# Model settings
model:
  model_id: "Qwen/Qwen2.5-0.5B-Instruct"
  trust_remote_code: true
  max_seq_len_to_capture: 100
  max_num_seqs: 1
  enable_lora: false # Set to true to enable LoRA adapters
  max_num_batched_tokens: 32768

# LoRA settings (optional)
# Set only if you want to use a specific LoRA adapter
# Note: If you enable LoRA, there is a known prompt-logprob bug (https://discuss.vllm.ai/t/bug-wrong-lora-mapping-during-prompt-logprobs-computing/500/2).
# lora:
#   lora_name: "adapter_name"
#   lora_int_id: 1
#   lora_path: "adapter_path"

# sampling_parameters settings
sampling_parameters:
  max_tokens: 1
  prompt_logprobs: 0
  temperature: 0.0
  top_p: 1.0

# Data settings
data:
  # Basic settings
  data_path: "./data/sample_data.csv"  # Path to data file or huggingface dataset name
  format: "csv"                        # Data format (csv, jsonl, json, parquet, huggingface)

  # Column name settings
  text_column: "input"               # Name of text column
  label_column: "label"              # Name of label column

  # text split settings
  text_length: 32                    # Number of words to split (for WikiMIA dataset: 32, 64, 128, or 256)

  # Language
  space_delimited_language: true

# Evaluation methods
methods:
  - type: "loss"
    params: {}
  - type: "lower"
    params: {}
  - type: "zlib"
    params: {}
  - type: "mink"
    params:
      ratio: 0.1
  - type: "mink"
    params:
      ratio: 0.2
  - type: "mink"
    params:
      ratio: 0.3
  - type: "mink"
    params:
      ratio: 0.5
  - type: "mink"
    params:
      ratio: 0.8
  - type: "mink"
    params:
      ratio: 1.0
  - type: "recall"
    params:
      num_shots: 3
      pass_window: false
  - type: "conrecall"
    params:
      num_shots: 3
      pass_window: false
      gamma: 0.5
  - type: "pac"
    params:
      alpha: 0.3
      N: 5
  # - type: "samia"
  #   params:
  #     num_samples: 5
  #     prefix_ratio: 0.5
  #     zlib: true

# Output settings
output_dir: "./results" 
