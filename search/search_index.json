{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Fast-MIA","text":"<p>Fast-MIA is a framework for efficiently evaluating Membership Inference Attacks (MIA) against Large Language Models (LLMs). This tool enables fast execution of representative membership inference methods using vLLM.</p>"},{"location":"#features","title":"\u2728 Features","text":"<ul> <li>\ud83d\ude80 Reduced Execution Time: Efficiently runs multiple inference methods using vLLM and result caching while preserving evaluation accuracy.</li> <li>\ud83d\udcca Cross-Method Evaluation: Compare and evaluate methods (LOSS, PPL/zlib, Min-K% Prob, etc.) under the same conditions.</li> <li>\ud83d\udd27 Flexibility &amp; Extensibility: Easily change models, datasets, evaluation methods, and parameters using YAML configuration files.</li> <li>\ud83c\udfaf Multiple Data Formats: Supports CSV, JSON, JSONL, Parquet, and Hugging Face Datasets.</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#environment","title":"Environment","text":"<p>Supported environments are Linux &amp; NVIDIA GPUs. It basically supports the same GPU requirements as vLLM. For example, it takes a few minutes to run using NVIDIA A100 80GB.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code># install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n# clone repository\ngit clone https://github.com/Nikkei/fast-mia.git\n# install dependencies\ncd fast-mia\nuv sync\nsource .venv/bin/activate\n</code></pre>"},{"location":"#execution","title":"Execution","text":"<pre><code>uv run --with 'vllm==0.15.1' python main.py --config config/sample.yaml\n</code></pre> <p>Note: When using T4 GPUs (e.g., Google Colab), set the environment variable to avoid attention backend issues:</p> <pre><code>VLLM_ATTENTION_BACKEND=XFORMERS uv run --with 'vllm==0.15.1' python main.py --config config/sample.yaml\n</code></pre> <p></p>"},{"location":"#detailed-report-mode","title":"Detailed Report Mode","text":"<p>For benchmarking with detailed outputs (metadata, per-sample scores, visualizations):</p> <pre><code>uv run --with 'vllm==0.15.1' python main.py --config config/sample.yaml --detailed-report\n</code></pre>"},{"location":"#supported-mia-methods","title":"\ud83d\udcda Supported MIA Methods","text":"<p>Fast-MIA supports the following MIA methods:</p> Type Method Name (identifier) Description Baseline LOSS (<code>loss</code>) Uses the model's loss PPL/zlib (<code>zlib</code>) Uses the ratio of information content calculated by Zlib compression Token distribution Min-K% Prob (<code>mink</code>) https://github.com/swj0419/detect-pretrain-code DC-PDD (<code>dcpdd</code>) https://github.com/zhang-wei-chao/DC-PDD Text alternation Lowercase (<code>lower</code>) Uses the ratio of loss after lowercasing the text PAC (<code>pac</code>) https://github.com/yyy01/PAC ReCaLL (<code>recall</code>) https://github.com/ruoyuxie/recall Con-ReCall (<code>conrecall</code>) https://github.com/WangCheng0116/CON-RECALL Black-box SaMIA (<code>samia</code>) https://github.com/nlp-titech/samia"},{"location":"#documentation","title":"\ud83d\udcd6 Documentation","text":"<ul> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"#links","title":"\ud83d\udd17 Links","text":"<ul> <li>GitHub Repository</li> <li>Issue Tracker</li> </ul>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>This project is licensed under the Apache License 2.0 - see the LICENSE file for details.</p>"},{"location":"#reference","title":"\ud83d\udcd1 Reference","text":"<pre><code>@misc{takahashi_ishihara_fastmia,\n  Author = {Hiromu Takahashi and Shotaro Ishihara},\n  Title = {{Fast-MIA}: Efficient and Scalable Membership Inference for LLMs},\n  Year = {2025},\n  Eprint = {arXiv:2510.23074},\n  URL = {https://arxiv.org/abs/2510.23074}\n}\n</code></pre>"},{"location":"api-reference/","title":"API Reference","text":""},{"location":"api-reference/#core-modules","title":"Core Modules","text":""},{"location":"api-reference/#config","title":"config","text":"<p>Classes:</p> <ul> <li> <code>Config</code>           \u2013            <p>Class for loading config files</p> </li> </ul>"},{"location":"api-reference/#config.Config","title":"Config","text":"<pre><code>Config(config_path: str | Path)\n</code></pre> <p>Class for loading config files</p> <p>Parameters:</p> <p>Attributes:</p> <ul> <li> <code>data</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Get data config</p> </li> <li> <code>lora</code>               (<code>dict[str, Any] | None</code>)           \u2013            <p>Get LoRA config</p> </li> <li> <code>methods</code>               (<code>list[dict[str, Any]]</code>)           \u2013            <p>Get method configs</p> </li> <li> <code>model</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Get model config</p> </li> <li> <code>sampling_parameters</code>               (<code>dict[str, Any]</code>)           \u2013            <p>Get sampling_parameters config</p> </li> </ul>"},{"location":"api-reference/#config.Config(config_path)","title":"<code>config_path</code>","text":"(<code>str | Path</code>)           \u2013            <p>Path to config file</p>"},{"location":"api-reference/#config.Config.data","title":"data  <code>property</code>","text":"<pre><code>data: dict[str, Any]\n</code></pre> <p>Get data config</p>"},{"location":"api-reference/#config.Config.lora","title":"lora  <code>property</code>","text":"<pre><code>lora: dict[str, Any] | None\n</code></pre> <p>Get LoRA config</p>"},{"location":"api-reference/#config.Config.methods","title":"methods  <code>property</code>","text":"<pre><code>methods: list[dict[str, Any]]\n</code></pre> <p>Get method configs</p>"},{"location":"api-reference/#config.Config.model","title":"model  <code>property</code>","text":"<pre><code>model: dict[str, Any]\n</code></pre> <p>Get model config</p>"},{"location":"api-reference/#config.Config.sampling_parameters","title":"sampling_parameters  <code>property</code>","text":"<pre><code>sampling_parameters: dict[str, Any]\n</code></pre> <p>Get sampling_parameters config</p>"},{"location":"api-reference/#data_loader","title":"data_loader","text":"<p>Classes:</p> <ul> <li> <code>DataLoader</code>           \u2013            <p>Data loader class</p> </li> </ul>"},{"location":"api-reference/#data_loader.DataLoader","title":"DataLoader","text":"<pre><code>DataLoader(\n    data_path: str | Path | None = None,\n    data_format: str = \"csv\",\n    text_column: str = \"text\",\n    label_column: str = \"label\",\n)\n</code></pre> <p>Data loader class</p> <p>Parameters:</p> Returned by: <ul> <li> API Reference <code></code>\u00a0data_loader <code></code>\u00a0DataLoader <ul> <li> <code></code>\u00a0load_mimir </li> <li> <code></code>\u00a0load_wikimia </li> </ul> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_data</code>             \u2013              <p>Get data</p> </li> <li> <code>load_mimir</code>             \u2013              <p>Load Mimir dataset with fixed text length constraints</p> </li> <li> <code>load_wikimia</code>             \u2013              <p>Load WikiMIA dataset with specified text length</p> </li> </ul>"},{"location":"api-reference/#data_loader.DataLoader(data_path)","title":"<code>data_path</code>","text":"(<code>str | Path | None</code>, default:                   <code>None</code> )           \u2013            <p>Path to the data (file or directory, or dataset name for huggingface format)</p>"},{"location":"api-reference/#data_loader.DataLoader(data_format)","title":"<code>data_format</code>","text":"(<code>str</code>, default:                   <code>'csv'</code> )           \u2013            <p>Data format (\"csv\", \"jsonl\", \"json\", \"parquet\", \"huggingface\")</p>"},{"location":"api-reference/#data_loader.DataLoader(text_column)","title":"<code>text_column</code>","text":"(<code>str</code>, default:                   <code>'text'</code> )           \u2013            <p>Name of the text column</p>"},{"location":"api-reference/#data_loader.DataLoader(label_column)","title":"<code>label_column</code>","text":"(<code>str</code>, default:                   <code>'label'</code> )           \u2013            <p>Name of the label column</p>"},{"location":"api-reference/#data_loader.DataLoader.get_data","title":"get_data","text":"<pre><code>get_data(text_length: int | None = None) -&gt; tuple[list[str], list[int]]\n</code></pre> <p>Get data</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>texts</code> (              <code>list[str]</code> )          \u2013            <p>List of texts</p> </li> <li> <code>labels</code> (              <code>list[int]</code> )          \u2013            <p>List of labels</p> </li> </ul>"},{"location":"api-reference/#data_loader.DataLoader.get_data(text_length)","title":"<code>text_length</code>","text":"(<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of words to split (if None, no split)</p>"},{"location":"api-reference/#data_loader.DataLoader.load_mimir","title":"load_mimir  <code>staticmethod</code>","text":"<pre><code>load_mimir(data_path: str, token: str) -&gt; DataLoader\n</code></pre> <p>Load Mimir dataset with fixed text length constraints</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataLoader</code>           \u2013            <p>DataLoader instance</p> </li> </ul>"},{"location":"api-reference/#data_loader.DataLoader.load_mimir(data_path)","title":"<code>data_path</code>","text":"(<code>str</code>)           \u2013            <p>Path to the data (dataset name for huggingface format)</p>"},{"location":"api-reference/#data_loader.DataLoader.load_mimir(token)","title":"<code>token</code>","text":"(<code>str</code>)           \u2013            <p>Hugging Face token</p>"},{"location":"api-reference/#data_loader.DataLoader.load_wikimia","title":"load_wikimia  <code>staticmethod</code>","text":"<pre><code>load_wikimia(text_length: int) -&gt; DataLoader\n</code></pre> <p>Load WikiMIA dataset with specified text length</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>DataLoader</code>           \u2013            <p>DataLoader instance</p> </li> </ul>"},{"location":"api-reference/#data_loader.DataLoader.load_wikimia(text_length)","title":"<code>text_length</code>","text":"(<code>int</code>)           \u2013            <p>Text length (one of 32, 64, 128, 256)</p>"},{"location":"api-reference/#evaluator","title":"evaluator","text":"<p>Classes:</p> <ul> <li> <code>EvaluationResult</code>           \u2013            <p>Container for evaluation results with detailed information</p> </li> <li> <code>Evaluator</code>           \u2013            <p>Evaluator for membership inference attacks</p> </li> </ul>"},{"location":"api-reference/#evaluator.EvaluationResult","title":"EvaluationResult  <code>dataclass</code>","text":"<pre><code>EvaluationResult(\n    results_df: DataFrame,\n    detailed_results: list[dict[str, Any]],\n    labels: list[int],\n    data_stats: dict[str, Any],\n    cache_stats: dict[str, Any] = dict(),\n)\n</code></pre> <p>Container for evaluation results with detailed information</p> Returned by: <ul> <li> API Reference <code></code>\u00a0evaluator <code></code>\u00a0Evaluator <code></code>\u00a0evaluate </li> </ul>"},{"location":"api-reference/#evaluator.Evaluator","title":"Evaluator","text":"<pre><code>Evaluator(\n    data_loader: DataLoader,\n    model_loader: ModelLoader,\n    methods: list[BaseMethod],\n    max_cache_size: int = 1000,\n)\n</code></pre> <p>Evaluator for membership inference attacks</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>evaluate</code>             \u2013              <p>Evaluate membership inference attacks on data with specified number of words</p> </li> </ul>"},{"location":"api-reference/#evaluator.Evaluator(data_loader)","title":"<code>data_loader</code>","text":"(<code>DataLoader</code>)           \u2013            <p>Data loader</p>"},{"location":"api-reference/#evaluator.Evaluator(model_loader)","title":"<code>model_loader</code>","text":"(<code>ModelLoader</code>)           \u2013            <p>Model loader</p>"},{"location":"api-reference/#evaluator.Evaluator(methods)","title":"<code>methods</code>","text":"(<code>list[BaseMethod]</code>)           \u2013            <p>List of methods to use for evaluation</p>"},{"location":"api-reference/#evaluator.Evaluator(max_cache_size)","title":"<code>max_cache_size</code>","text":"(<code>int</code>, default:                   <code>1000</code> )           \u2013            <p>Maximum cache size</p>"},{"location":"api-reference/#evaluator.Evaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(config: Config) -&gt; EvaluationResult\n</code></pre> <p>Evaluate membership inference attacks on data with specified number of words</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>EvaluationResult</code>           \u2013            <p>EvaluationResult containing DataFrame, detailed results, labels, and stats</p> </li> </ul>"},{"location":"api-reference/#evaluator.Evaluator.evaluate(config)","title":"<code>config</code>","text":"(<code>Config</code>)           \u2013            <p>Configuration</p>"},{"location":"api-reference/#model_loader","title":"model_loader","text":"<p>Classes:</p> <ul> <li> <code>ModelLoader</code>           \u2013            <p>vLLM model loader class</p> </li> </ul>"},{"location":"api-reference/#model_loader.ModelLoader","title":"ModelLoader","text":"<pre><code>ModelLoader(model_config: dict[str, Any])\n</code></pre> <p>vLLM model loader class</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>get_lora_request</code>             \u2013              <p>Get LoRA request</p> </li> <li> <code>get_sampling_params</code>             \u2013              <p>Get sampling parameters</p> </li> </ul>"},{"location":"api-reference/#model_loader.ModelLoader(model_config)","title":"<code>model_config</code>","text":"(<code>dict[str, Any]</code>)           \u2013            <p>Model configuration</p>"},{"location":"api-reference/#model_loader.ModelLoader.get_lora_request","title":"get_lora_request","text":"<pre><code>get_lora_request(lora_config: dict[str, Any]) -&gt; LoRARequest\n</code></pre> <p>Get LoRA request</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>LoRARequest</code>           \u2013            <p>LoRA request</p> </li> </ul>"},{"location":"api-reference/#model_loader.ModelLoader.get_lora_request(lora_config)","title":"<code>lora_config</code>","text":"(<code>dict[str, Any]</code>)           \u2013            <p>LoRA configuration</p>"},{"location":"api-reference/#model_loader.ModelLoader.get_sampling_params","title":"get_sampling_params","text":"<pre><code>get_sampling_params(sampling_parameters: dict[str, Any]) -&gt; SamplingParams\n</code></pre> <p>Get sampling parameters</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>SamplingParams</code>           \u2013            <p>Sampling parameters</p> </li> </ul>"},{"location":"api-reference/#model_loader.ModelLoader.get_sampling_params(sampling_parameters)","title":"<code>sampling_parameters</code>","text":"(<code>dict[str, Any]</code>)           \u2013            <p>Sampling parameters configuration</p>"},{"location":"api-reference/#utils","title":"utils","text":"<p>Functions:</p> <ul> <li> <code>fix_seed</code>             \u2013              <p>Fix random seed</p> </li> <li> <code>get_metrics</code>             \u2013              <p>Calculate evaluation metrics</p> </li> </ul>"},{"location":"api-reference/#utils.fix_seed","title":"fix_seed","text":"<pre><code>fix_seed(seed: int = 0) -&gt; None\n</code></pre> <p>Fix random seed</p> <p>Parameters:</p>"},{"location":"api-reference/#utils.fix_seed(seed)","title":"<code>seed</code>","text":"(<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Seed value to fix</p>"},{"location":"api-reference/#utils.get_metrics","title":"get_metrics","text":"<pre><code>get_metrics(scores: list[float], labels: list[int]) -&gt; tuple[float, float, float]\n</code></pre> <p>Calculate evaluation metrics</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>auroc</code> (              <code>float</code> )          \u2013            <p>AUROC</p> </li> <li> <code>fpr95</code> (              <code>float</code> )          \u2013            <p>FPR when TPR is 95%</p> </li> <li> <code>tpr05</code> (              <code>float</code> )          \u2013            <p>TPR when FPR is 5%</p> </li> </ul>"},{"location":"api-reference/#utils.get_metrics(scores)","title":"<code>scores</code>","text":"(<code>list[float]</code>)           \u2013            <p>List of scores</p>"},{"location":"api-reference/#utils.get_metrics(labels)","title":"<code>labels</code>","text":"(<code>list[int]</code>)           \u2013            <p>List of labels (1: membership, 0: non-membership)</p>"},{"location":"api-reference/#mia-methods","title":"MIA Methods","text":""},{"location":"api-reference/#methods","title":"methods","text":"<p>Modules:</p> <ul> <li> <code>base</code>           \u2013            </li> <li> <code>conrecall</code>           \u2013            </li> <li> <code>dcpdd</code>           \u2013            </li> <li> <code>factory</code>           \u2013            </li> <li> <code>loss</code>           \u2013            </li> <li> <code>lower</code>           \u2013            </li> <li> <code>mink</code>           \u2013            </li> <li> <code>pac</code>           \u2013            </li> <li> <code>prefix_utils</code>           \u2013            </li> <li> <code>recall</code>           \u2013            </li> <li> <code>samia</code>           \u2013            </li> <li> <code>zlib</code>           \u2013            </li> </ul> <p>Classes:</p> <ul> <li> <code>BaseMethod</code>           \u2013            <p>Base class for membership inference methods</p> </li> <li> <code>CONReCaLLMethod</code>           \u2013            <p>Con-ReCall membership inference method</p> </li> <li> <code>DCPDDMethod</code>           \u2013            <p>DC-PDD membership inference method</p> </li> <li> <code>LossMethod</code>           \u2013            <p>Loss (log-likelihood) based membership inference method</p> </li> <li> <code>LowerMethod</code>           \u2013            <p>Lower based membership inference method</p> </li> <li> <code>MethodFactory</code>           \u2013            <p>Method factory class</p> </li> <li> <code>MinKMethod</code>           \u2013            <p>Min-K% Prob based membership inference method</p> </li> <li> <code>PACMethod</code>           \u2013            <p>PAC (Polarized Augment Calibration) based membership inference method</p> </li> <li> <code>ReCaLLMethod</code>           \u2013            <p>ReCaLL membership inference method</p> </li> <li> <code>SaMIAMethod</code>           \u2013            <p>SaMIA membership inference method</p> </li> <li> <code>ZlibMethod</code>           \u2013            <p>Zlib compression-based membership inference method</p> </li> </ul>"},{"location":"api-reference/#methods.BaseMethod","title":"BaseMethod","text":"<pre><code>BaseMethod(method_name: str, method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.BaseMethod[BaseMethod]\n\n              \n\n              click methods.BaseMethod href \"\" \"methods.BaseMethod\"\n            </code></pre> <p>Base class for membership inference methods</p> <p>Parameters:</p> Returned by: <ul> <li> API Reference <code></code>\u00a0methods <ul> <li> <code></code>\u00a0MethodFactory <ul> <li> <code></code>\u00a0create_method </li> <li> <code></code>\u00a0create_methods </li> </ul> </li> <li> <code></code>\u00a0factory <code></code>\u00a0MethodFactory <ul> <li> <code></code>\u00a0create_method </li> <li> <code></code>\u00a0create_methods </li> </ul> </li> </ul> </li> </ul> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate score</p> </li> <li> <code>run</code>             \u2013              <p>Run inference for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.BaseMethod(method_name)","title":"<code>method_name</code>","text":"(<code>str</code>)           \u2013            <p>Name of the method</p>"},{"location":"api-reference/#methods.BaseMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.BaseMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.BaseMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.BaseMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.BaseMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.BaseMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.BaseMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.BaseMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.BaseMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.BaseMethod.process_output","title":"process_output  <code>abstractmethod</code>","text":"<pre><code>process_output(output: RequestOutput) -&gt; float\n</code></pre> <p>Process model output and calculate score</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Score</p> </li> </ul>"},{"location":"api-reference/#methods.BaseMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.BaseMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>Run inference for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of scores</p> </li> </ul>"},{"location":"api-reference/#methods.BaseMethod.run(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.BaseMethod.run(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.BaseMethod.run(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.BaseMethod.run(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.BaseMethod.run(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.BaseMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.BaseMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.CONReCaLLMethod","title":"CONReCaLLMethod","text":"<pre><code>CONReCaLLMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.CONReCaLLMethod[CONReCaLLMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.CONReCaLLMethod\n                \n\n\n              click methods.CONReCaLLMethod href \"\" \"methods.CONReCaLLMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>Con-ReCall membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate loss (negative log-likelihood)</p> </li> <li> <code>run</code>             \u2013              <p>CON-ReCaLL algorithm to calculate scores for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.CONReCaLLMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.CONReCaLLMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.CONReCaLLMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.CONReCaLLMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.CONReCaLLMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.CONReCaLLMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.CONReCaLLMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.CONReCaLLMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.CONReCaLLMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.CONReCaLLMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput, prefix_token_length: int) -&gt; float\n</code></pre> <p>Process model output and calculate loss (negative log-likelihood)</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Loss</p> </li> </ul>"},{"location":"api-reference/#methods.CONReCaLLMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.CONReCaLLMethod.process_output(prefix_token_length)","title":"<code>prefix_token_length</code>","text":"(<code>int</code>)           \u2013            <p>Number of prefix tokens to exclude</p>"},{"location":"api-reference/#methods.CONReCaLLMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    labels: list[int],\n    model: LLM,\n    tokenizer: AnyTokenizer,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>CON-ReCaLL algorithm to calculate scores for a list of texts Args:     texts: List of texts     labels: List of labels     model: LLM model     tokenizer: Tokenizer     sampling_params: Sampling parameters     lora_request: LoRA request     data_config: Data configuration</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of CON-ReCaLL scores</p> </li> </ul>"},{"location":"api-reference/#methods.CONReCaLLMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.CONReCaLLMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.DCPDDMethod","title":"DCPDDMethod","text":"<pre><code>DCPDDMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.DCPDDMethod[DCPDDMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.DCPDDMethod\n                \n\n\n              click methods.DCPDDMethod href \"\" \"methods.DCPDDMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>DC-PDD membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate DC-PDD score</p> </li> <li> <code>run</code>             \u2013              <p>DC-PDD algorithm to calculate scores for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.DCPDDMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.DCPDDMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.DCPDDMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.DCPDDMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.DCPDDMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.DCPDDMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.DCPDDMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.DCPDDMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.DCPDDMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.DCPDDMethod.process_output","title":"process_output","text":"<pre><code>process_output(\n    output: RequestOutput, input_ids: list[int], freq_dist: list[int]\n) -&gt; float\n</code></pre> <p>Process model output and calculate DC-PDD score</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>DC-PDD score</p> </li> </ul>"},{"location":"api-reference/#methods.DCPDDMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.DCPDDMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    tokenizer: AnyTokenizer,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>DC-PDD algorithm to calculate scores for a list of texts Args:     texts: List of texts     labels: List of labels     model: LLM model     tokenizer: Tokenizer     sampling_params: Sampling parameters     lora_request: LoRA request     data_config: Data configuration</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of DC-PDD scores</p> </li> </ul>"},{"location":"api-reference/#methods.DCPDDMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.DCPDDMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.LossMethod","title":"LossMethod","text":"<pre><code>LossMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.LossMethod[LossMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.LossMethod\n                \n\n\n              click methods.LossMethod href \"\" \"methods.LossMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>Loss (log-likelihood) based membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate loss</p> </li> <li> <code>run</code>             \u2013              <p>Run inference for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.LossMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.LossMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.LossMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.LossMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.LossMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.LossMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.LossMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.LossMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.LossMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.LossMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput) -&gt; float\n</code></pre> <p>Process model output and calculate loss</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Loss</p> </li> </ul>"},{"location":"api-reference/#methods.LossMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.LossMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>Run inference for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of scores</p> </li> </ul>"},{"location":"api-reference/#methods.LossMethod.run(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.LossMethod.run(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.LossMethod.run(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.LossMethod.run(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.LossMethod.run(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.LossMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.LossMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.LowerMethod","title":"LowerMethod","text":"<pre><code>LowerMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.LowerMethod[LowerMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.LowerMethod\n                \n\n\n              click methods.LowerMethod href \"\" \"methods.LowerMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>Lower based membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate loss</p> </li> <li> <code>run</code>             \u2013              <p>Run Lower algorithm and calculate scores for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.LowerMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.LowerMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.LowerMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.LowerMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.LowerMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.LowerMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.LowerMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.LowerMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.LowerMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.LowerMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput) -&gt; float\n</code></pre> <p>Process model output and calculate loss</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Negative mean log-likelihood (loss)</p> </li> </ul>"},{"location":"api-reference/#methods.LowerMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.LowerMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>Run Lower algorithm and calculate scores for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of scores</p> </li> </ul>"},{"location":"api-reference/#methods.LowerMethod.run(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.LowerMethod.run(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.LowerMethod.run(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.LowerMethod.run(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.LowerMethod.run(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.LowerMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.LowerMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.MethodFactory","title":"MethodFactory","text":"<p>Method factory class</p> <p>Methods:</p> <ul> <li> <code>create_method</code>             \u2013              <p>Create a method</p> </li> <li> <code>create_methods</code>             \u2013              <p>Create multiple methods</p> </li> </ul>"},{"location":"api-reference/#methods.MethodFactory.create_method","title":"create_method  <code>staticmethod</code>","text":"<pre><code>create_method(method_config: dict[str, Any]) -&gt; BaseMethod\n</code></pre> <p>Create a method</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>BaseMethod</code>           \u2013            <p>Created method</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If unknown method type is specified</p> </li> </ul>"},{"location":"api-reference/#methods.MethodFactory.create_method(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>)           \u2013            <p>Method configuration - type: Type of method ('loss', 'lower', 'zlib', 'mink', 'pac', 'recall', 'conrecall', 'samia', 'dcpdd') - params: Method-specific parameters</p>"},{"location":"api-reference/#methods.MethodFactory.create_methods","title":"create_methods  <code>staticmethod</code>","text":"<pre><code>create_methods(methods_config: list[dict[str, Any]]) -&gt; list[BaseMethod]\n</code></pre> <p>Create multiple methods</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[BaseMethod]</code>           \u2013            <p>List of created methods</p> </li> </ul>"},{"location":"api-reference/#methods.MethodFactory.create_methods(methods_config)","title":"<code>methods_config</code>","text":"(<code>list[dict[str, Any]]</code>)           \u2013            <p>List of method configurations</p>"},{"location":"api-reference/#methods.MinKMethod","title":"MinKMethod","text":"<pre><code>MinKMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.MinKMethod[MinKMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.MinKMethod\n                \n\n\n              click methods.MinKMethod href \"\" \"methods.MinKMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>Min-K% Prob based membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate Min-K% score</p> </li> <li> <code>run</code>             \u2013              <p>Run inference for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.MinKMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration - ratio: Ratio of lowest probability tokens to use (0.0-1.0)</p>"},{"location":"api-reference/#methods.MinKMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.MinKMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.MinKMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.MinKMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.MinKMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.MinKMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.MinKMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.MinKMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.MinKMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput) -&gt; float\n</code></pre> <p>Process model output and calculate Min-K% score</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Min-K% score</p> </li> </ul>"},{"location":"api-reference/#methods.MinKMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.MinKMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>Run inference for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of scores</p> </li> </ul>"},{"location":"api-reference/#methods.MinKMethod.run(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.MinKMethod.run(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.MinKMethod.run(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.MinKMethod.run(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.MinKMethod.run(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.MinKMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.MinKMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.PACMethod","title":"PACMethod","text":"<pre><code>PACMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.PACMethod[PACMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.PACMethod\n                \n\n\n              click methods.PACMethod href \"\" \"methods.PACMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>PAC (Polarized Augment Calibration) based membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate Polarized Distance</p> </li> <li> <code>run</code>             \u2013              <p>PAC algorithm to calculate scores for a list of texts.</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.PACMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.PACMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.PACMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.PACMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.PACMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.PACMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.PACMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.PACMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.PACMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.PACMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput) -&gt; float\n</code></pre> <p>Process model output and calculate Polarized Distance</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Polarized Distance</p> </li> </ul>"},{"location":"api-reference/#methods.PACMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.PACMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>PAC algorithm to calculate scores for a list of texts. Args:     texts: List of texts     model: LLM model     sampling_params: Sampling parameters     lora_request: LoRA request     data_config: Data configuration</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of PAC scores</p> </li> </ul>"},{"location":"api-reference/#methods.PACMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.PACMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.ReCaLLMethod","title":"ReCaLLMethod","text":"<pre><code>ReCaLLMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.ReCaLLMethod[ReCaLLMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.ReCaLLMethod\n                \n\n\n              click methods.ReCaLLMethod href \"\" \"methods.ReCaLLMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>ReCaLL membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate loss (negative log-likelihood)</p> </li> <li> <code>run</code>             \u2013              <p>ReCaLL algorithm to calculate scores for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.ReCaLLMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.ReCaLLMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.ReCaLLMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.ReCaLLMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.ReCaLLMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.ReCaLLMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.ReCaLLMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.ReCaLLMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.ReCaLLMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.ReCaLLMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput, prefix_token_length: int) -&gt; float\n</code></pre> <p>Process model output and calculate loss (negative log-likelihood)</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Loss</p> </li> </ul>"},{"location":"api-reference/#methods.ReCaLLMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.ReCaLLMethod.process_output(prefix_token_length)","title":"<code>prefix_token_length</code>","text":"(<code>int</code>)           \u2013            <p>Number of prefix tokens to exclude</p>"},{"location":"api-reference/#methods.ReCaLLMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    labels: list[int],\n    model: LLM,\n    tokenizer: AnyTokenizer,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>ReCaLL algorithm to calculate scores for a list of texts Args:     texts: List of texts     labels: List of labels     model: LLM model     tokenizer: Tokenizer     sampling_params: Sampling parameters     lora_request: LoRA request     data_config: Data configuration</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of ReCaLL scores</p> </li> </ul>"},{"location":"api-reference/#methods.ReCaLLMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.ReCaLLMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.SaMIAMethod","title":"SaMIAMethod","text":"<pre><code>SaMIAMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.SaMIAMethod[SaMIAMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.SaMIAMethod\n                \n\n\n              click methods.SaMIAMethod href \"\" \"methods.SaMIAMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>SaMIA membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Calculate SaMIA score from a single model output</p> </li> <li> <code>run</code>             \u2013              <p>SaMIA algorithm to calculate scores for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.SaMIAMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.SaMIAMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.SaMIAMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.SaMIAMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.SaMIAMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.SaMIAMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.SaMIAMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.SaMIAMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.SaMIAMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.SaMIAMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput) -&gt; float\n</code></pre> <p>Calculate SaMIA score from a single model output Note: This method is called from BaseMethod.run, but for SaMIA, a custom implementation using multiple samples is used, so this method is not supported for single output. Use run method instead.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>SaMIA score</p> </li> </ul>"},{"location":"api-reference/#methods.SaMIAMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.SaMIAMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>SaMIA algorithm to calculate scores for a list of texts Args:     texts: List of texts     model: LLM model     lora_request: LoRA request     data_config: Data configuration</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of SaMIA scores</p> </li> </ul>"},{"location":"api-reference/#methods.SaMIAMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.SaMIAMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.ZlibMethod","title":"ZlibMethod","text":"<pre><code>ZlibMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.ZlibMethod[ZlibMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.ZlibMethod\n                \n\n\n              click methods.ZlibMethod href \"\" \"methods.ZlibMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>Zlib compression-based membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate zlib-compressed information content ratio</p> </li> <li> <code>run</code>             \u2013              <p>Run inference for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.ZlibMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.ZlibMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.ZlibMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.ZlibMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.ZlibMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.ZlibMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.ZlibMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.ZlibMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.ZlibMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.ZlibMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput) -&gt; float\n</code></pre> <p>Process model output and calculate zlib-compressed information content ratio</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>zlib-compressed information content ratio</p> </li> </ul>"},{"location":"api-reference/#methods.ZlibMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.ZlibMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>Run inference for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of scores</p> </li> </ul>"},{"location":"api-reference/#methods.ZlibMethod.run(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.ZlibMethod.run(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.ZlibMethod.run(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.ZlibMethod.run(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.ZlibMethod.run(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.ZlibMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.ZlibMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.base","title":"base","text":"<p>Classes:</p> <ul> <li> <code>BaseMethod</code>           \u2013            <p>Base class for membership inference methods</p> </li> </ul>"},{"location":"api-reference/#methods.base.BaseMethod","title":"BaseMethod","text":"<pre><code>BaseMethod(method_name: str, method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.base.BaseMethod[BaseMethod]\n\n              \n\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>Base class for membership inference methods</p> <p>Parameters:</p> Returned by: <ul> <li> API Reference <code></code>\u00a0methods <ul> <li> <code></code>\u00a0MethodFactory <ul> <li> <code></code>\u00a0create_method </li> <li> <code></code>\u00a0create_methods </li> </ul> </li> <li> <code></code>\u00a0factory <code></code>\u00a0MethodFactory <ul> <li> <code></code>\u00a0create_method </li> <li> <code></code>\u00a0create_methods </li> </ul> </li> </ul> </li> </ul> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate score</p> </li> <li> <code>run</code>             \u2013              <p>Run inference for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.base.BaseMethod(method_name)","title":"<code>method_name</code>","text":"(<code>str</code>)           \u2013            <p>Name of the method</p>"},{"location":"api-reference/#methods.base.BaseMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.base.BaseMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.base.BaseMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.base.BaseMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.base.BaseMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.base.BaseMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.base.BaseMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.base.BaseMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.base.BaseMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.base.BaseMethod.process_output","title":"process_output  <code>abstractmethod</code>","text":"<pre><code>process_output(output: RequestOutput) -&gt; float\n</code></pre> <p>Process model output and calculate score</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Score</p> </li> </ul>"},{"location":"api-reference/#methods.base.BaseMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.base.BaseMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>Run inference for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of scores</p> </li> </ul>"},{"location":"api-reference/#methods.base.BaseMethod.run(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.base.BaseMethod.run(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.base.BaseMethod.run(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.base.BaseMethod.run(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.base.BaseMethod.run(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.base.BaseMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.base.BaseMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.conrecall","title":"conrecall","text":"<p>Classes:</p> <ul> <li> <code>CONReCaLLMethod</code>           \u2013            <p>Con-ReCall membership inference method</p> </li> </ul>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod","title":"CONReCaLLMethod","text":"<pre><code>CONReCaLLMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.conrecall.CONReCaLLMethod[CONReCaLLMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.conrecall.CONReCaLLMethod\n                \n\n\n              click methods.conrecall.CONReCaLLMethod href \"\" \"methods.conrecall.CONReCaLLMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>Con-ReCall membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate loss (negative log-likelihood)</p> </li> <li> <code>run</code>             \u2013              <p>CON-ReCaLL algorithm to calculate scores for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput, prefix_token_length: int) -&gt; float\n</code></pre> <p>Process model output and calculate loss (negative log-likelihood)</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Loss</p> </li> </ul>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod.process_output(prefix_token_length)","title":"<code>prefix_token_length</code>","text":"(<code>int</code>)           \u2013            <p>Number of prefix tokens to exclude</p>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    labels: list[int],\n    model: LLM,\n    tokenizer: AnyTokenizer,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>CON-ReCaLL algorithm to calculate scores for a list of texts Args:     texts: List of texts     labels: List of labels     model: LLM model     tokenizer: Tokenizer     sampling_params: Sampling parameters     lora_request: LoRA request     data_config: Data configuration</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of CON-ReCaLL scores</p> </li> </ul>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.conrecall.CONReCaLLMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.dcpdd","title":"dcpdd","text":"<p>Classes:</p> <ul> <li> <code>DCPDDMethod</code>           \u2013            <p>DC-PDD membership inference method</p> </li> </ul>"},{"location":"api-reference/#methods.dcpdd.DCPDDMethod","title":"DCPDDMethod","text":"<pre><code>DCPDDMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.dcpdd.DCPDDMethod[DCPDDMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.dcpdd.DCPDDMethod\n                \n\n\n              click methods.dcpdd.DCPDDMethod href \"\" \"methods.dcpdd.DCPDDMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>DC-PDD membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate DC-PDD score</p> </li> <li> <code>run</code>             \u2013              <p>DC-PDD algorithm to calculate scores for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.dcpdd.DCPDDMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.dcpdd.DCPDDMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.dcpdd.DCPDDMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.dcpdd.DCPDDMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.dcpdd.DCPDDMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.dcpdd.DCPDDMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.dcpdd.DCPDDMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.dcpdd.DCPDDMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.dcpdd.DCPDDMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.dcpdd.DCPDDMethod.process_output","title":"process_output","text":"<pre><code>process_output(\n    output: RequestOutput, input_ids: list[int], freq_dist: list[int]\n) -&gt; float\n</code></pre> <p>Process model output and calculate DC-PDD score</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>DC-PDD score</p> </li> </ul>"},{"location":"api-reference/#methods.dcpdd.DCPDDMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.dcpdd.DCPDDMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    tokenizer: AnyTokenizer,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>DC-PDD algorithm to calculate scores for a list of texts Args:     texts: List of texts     labels: List of labels     model: LLM model     tokenizer: Tokenizer     sampling_params: Sampling parameters     lora_request: LoRA request     data_config: Data configuration</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of DC-PDD scores</p> </li> </ul>"},{"location":"api-reference/#methods.dcpdd.DCPDDMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.dcpdd.DCPDDMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.factory","title":"factory","text":"<p>Classes:</p> <ul> <li> <code>MethodFactory</code>           \u2013            <p>Method factory class</p> </li> </ul>"},{"location":"api-reference/#methods.factory.MethodFactory","title":"MethodFactory","text":"<p>Method factory class</p> <p>Methods:</p> <ul> <li> <code>create_method</code>             \u2013              <p>Create a method</p> </li> <li> <code>create_methods</code>             \u2013              <p>Create multiple methods</p> </li> </ul>"},{"location":"api-reference/#methods.factory.MethodFactory.create_method","title":"create_method  <code>staticmethod</code>","text":"<pre><code>create_method(method_config: dict[str, Any]) -&gt; BaseMethod\n</code></pre> <p>Create a method</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>BaseMethod</code>           \u2013            <p>Created method</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If unknown method type is specified</p> </li> </ul>"},{"location":"api-reference/#methods.factory.MethodFactory.create_method(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>)           \u2013            <p>Method configuration - type: Type of method ('loss', 'lower', 'zlib', 'mink', 'pac', 'recall', 'conrecall', 'samia', 'dcpdd') - params: Method-specific parameters</p>"},{"location":"api-reference/#methods.factory.MethodFactory.create_methods","title":"create_methods  <code>staticmethod</code>","text":"<pre><code>create_methods(methods_config: list[dict[str, Any]]) -&gt; list[BaseMethod]\n</code></pre> <p>Create multiple methods</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[BaseMethod]</code>           \u2013            <p>List of created methods</p> </li> </ul>"},{"location":"api-reference/#methods.factory.MethodFactory.create_methods(methods_config)","title":"<code>methods_config</code>","text":"(<code>list[dict[str, Any]]</code>)           \u2013            <p>List of method configurations</p>"},{"location":"api-reference/#methods.loss","title":"loss","text":"<p>Classes:</p> <ul> <li> <code>LossMethod</code>           \u2013            <p>Loss (log-likelihood) based membership inference method</p> </li> </ul>"},{"location":"api-reference/#methods.loss.LossMethod","title":"LossMethod","text":"<pre><code>LossMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.loss.LossMethod[LossMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.loss.LossMethod\n                \n\n\n              click methods.loss.LossMethod href \"\" \"methods.loss.LossMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>Loss (log-likelihood) based membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate loss</p> </li> <li> <code>run</code>             \u2013              <p>Run inference for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.loss.LossMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.loss.LossMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.loss.LossMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.loss.LossMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.loss.LossMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.loss.LossMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.loss.LossMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.loss.LossMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.loss.LossMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.loss.LossMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput) -&gt; float\n</code></pre> <p>Process model output and calculate loss</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Loss</p> </li> </ul>"},{"location":"api-reference/#methods.loss.LossMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.loss.LossMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>Run inference for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of scores</p> </li> </ul>"},{"location":"api-reference/#methods.loss.LossMethod.run(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.loss.LossMethod.run(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.loss.LossMethod.run(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.loss.LossMethod.run(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.loss.LossMethod.run(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.loss.LossMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.loss.LossMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.lower","title":"lower","text":"<p>Classes:</p> <ul> <li> <code>LowerMethod</code>           \u2013            <p>Lower based membership inference method</p> </li> </ul>"},{"location":"api-reference/#methods.lower.LowerMethod","title":"LowerMethod","text":"<pre><code>LowerMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.lower.LowerMethod[LowerMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.lower.LowerMethod\n                \n\n\n              click methods.lower.LowerMethod href \"\" \"methods.lower.LowerMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>Lower based membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate loss</p> </li> <li> <code>run</code>             \u2013              <p>Run Lower algorithm and calculate scores for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.lower.LowerMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.lower.LowerMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.lower.LowerMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.lower.LowerMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.lower.LowerMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.lower.LowerMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.lower.LowerMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.lower.LowerMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.lower.LowerMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.lower.LowerMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput) -&gt; float\n</code></pre> <p>Process model output and calculate loss</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Negative mean log-likelihood (loss)</p> </li> </ul>"},{"location":"api-reference/#methods.lower.LowerMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.lower.LowerMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>Run Lower algorithm and calculate scores for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of scores</p> </li> </ul>"},{"location":"api-reference/#methods.lower.LowerMethod.run(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.lower.LowerMethod.run(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.lower.LowerMethod.run(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.lower.LowerMethod.run(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.lower.LowerMethod.run(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.lower.LowerMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.lower.LowerMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.mink","title":"mink","text":"<p>Classes:</p> <ul> <li> <code>MinKMethod</code>           \u2013            <p>Min-K% Prob based membership inference method</p> </li> </ul>"},{"location":"api-reference/#methods.mink.MinKMethod","title":"MinKMethod","text":"<pre><code>MinKMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.mink.MinKMethod[MinKMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.mink.MinKMethod\n                \n\n\n              click methods.mink.MinKMethod href \"\" \"methods.mink.MinKMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>Min-K% Prob based membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate Min-K% score</p> </li> <li> <code>run</code>             \u2013              <p>Run inference for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.mink.MinKMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration - ratio: Ratio of lowest probability tokens to use (0.0-1.0)</p>"},{"location":"api-reference/#methods.mink.MinKMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.mink.MinKMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.mink.MinKMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.mink.MinKMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.mink.MinKMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.mink.MinKMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.mink.MinKMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.mink.MinKMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.mink.MinKMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput) -&gt; float\n</code></pre> <p>Process model output and calculate Min-K% score</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Min-K% score</p> </li> </ul>"},{"location":"api-reference/#methods.mink.MinKMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.mink.MinKMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>Run inference for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of scores</p> </li> </ul>"},{"location":"api-reference/#methods.mink.MinKMethod.run(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.mink.MinKMethod.run(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.mink.MinKMethod.run(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.mink.MinKMethod.run(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.mink.MinKMethod.run(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.mink.MinKMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.mink.MinKMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.pac","title":"pac","text":"<p>Classes:</p> <ul> <li> <code>PACMethod</code>           \u2013            <p>PAC (Polarized Augment Calibration) based membership inference method</p> </li> </ul>"},{"location":"api-reference/#methods.pac.PACMethod","title":"PACMethod","text":"<pre><code>PACMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.pac.PACMethod[PACMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.pac.PACMethod\n                \n\n\n              click methods.pac.PACMethod href \"\" \"methods.pac.PACMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>PAC (Polarized Augment Calibration) based membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate Polarized Distance</p> </li> <li> <code>run</code>             \u2013              <p>PAC algorithm to calculate scores for a list of texts.</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.pac.PACMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.pac.PACMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.pac.PACMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.pac.PACMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.pac.PACMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.pac.PACMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.pac.PACMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.pac.PACMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.pac.PACMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.pac.PACMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput) -&gt; float\n</code></pre> <p>Process model output and calculate Polarized Distance</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Polarized Distance</p> </li> </ul>"},{"location":"api-reference/#methods.pac.PACMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.pac.PACMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>PAC algorithm to calculate scores for a list of texts. Args:     texts: List of texts     model: LLM model     sampling_params: Sampling parameters     lora_request: LoRA request     data_config: Data configuration</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of PAC scores</p> </li> </ul>"},{"location":"api-reference/#methods.pac.PACMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.pac.PACMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.prefix_utils","title":"prefix_utils","text":"<p>Functions:</p> <ul> <li> <code>compute_prefix_loss</code>             \u2013              <p>Compute negative mean log-likelihood excluding prefix tokens.</p> </li> <li> <code>extract_prefix</code>             \u2013              <p>Randomly select num_shots texts from the list without modifying the original.</p> </li> <li> <code>process_prefix</code>             \u2013              <p>Process prefix to fit within model's max length.</p> </li> </ul>"},{"location":"api-reference/#methods.prefix_utils.compute_prefix_loss","title":"compute_prefix_loss","text":"<pre><code>compute_prefix_loss(output: RequestOutput, prefix_token_length: int) -&gt; float\n</code></pre> <p>Compute negative mean log-likelihood excluding prefix tokens.</p> <p>Shared by ReCaLL and CON-ReCaLL methods.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Negative mean log-likelihood (loss)</p> </li> </ul>"},{"location":"api-reference/#methods.prefix_utils.compute_prefix_loss(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.prefix_utils.compute_prefix_loss(prefix_token_length)","title":"<code>prefix_token_length</code>","text":"(<code>int</code>)           \u2013            <p>Number of prefix tokens to exclude</p>"},{"location":"api-reference/#methods.prefix_utils.extract_prefix","title":"extract_prefix","text":"<pre><code>extract_prefix(texts: list[str], num_shots: int) -&gt; list[str]\n</code></pre> <p>Randomly select num_shots texts from the list without modifying the original.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>List of randomly selected texts</p> </li> </ul>"},{"location":"api-reference/#methods.prefix_utils.extract_prefix(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts to sample from</p>"},{"location":"api-reference/#methods.prefix_utils.extract_prefix(num_shots)","title":"<code>num_shots</code>","text":"(<code>int</code>)           \u2013            <p>Number of texts to select</p>"},{"location":"api-reference/#methods.prefix_utils.process_prefix","title":"process_prefix","text":"<pre><code>process_prefix(\n    model: LLM,\n    tokenizer: AnyTokenizer,\n    prefix: list[str],\n    avg_length: int,\n    pass_window: bool,\n    num_shots: int,\n) -&gt; tuple[list[str], int]\n</code></pre> <p>Process prefix to fit within model's max length.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>tuple[list[str], int]</code>           \u2013            <p>Tuple of (processed prefix, actual number of shots)</p> </li> </ul>"},{"location":"api-reference/#methods.prefix_utils.process_prefix(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.prefix_utils.process_prefix(tokenizer)","title":"<code>tokenizer</code>","text":"(<code>AnyTokenizer</code>)           \u2013            <p>Tokenizer</p>"},{"location":"api-reference/#methods.prefix_utils.process_prefix(prefix)","title":"<code>prefix</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of prefix texts</p>"},{"location":"api-reference/#methods.prefix_utils.process_prefix(avg_length)","title":"<code>avg_length</code>","text":"(<code>int</code>)           \u2013            <p>Average token length of texts</p>"},{"location":"api-reference/#methods.prefix_utils.process_prefix(pass_window)","title":"<code>pass_window</code>","text":"(<code>bool</code>)           \u2013            <p>If True, skip window check</p>"},{"location":"api-reference/#methods.prefix_utils.process_prefix(num_shots)","title":"<code>num_shots</code>","text":"(<code>int</code>)           \u2013            <p>Number of shots</p>"},{"location":"api-reference/#methods.recall","title":"recall","text":"<p>Classes:</p> <ul> <li> <code>ReCaLLMethod</code>           \u2013            <p>ReCaLL membership inference method</p> </li> </ul>"},{"location":"api-reference/#methods.recall.ReCaLLMethod","title":"ReCaLLMethod","text":"<pre><code>ReCaLLMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.recall.ReCaLLMethod[ReCaLLMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.recall.ReCaLLMethod\n                \n\n\n              click methods.recall.ReCaLLMethod href \"\" \"methods.recall.ReCaLLMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>ReCaLL membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate loss (negative log-likelihood)</p> </li> <li> <code>run</code>             \u2013              <p>ReCaLL algorithm to calculate scores for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.recall.ReCaLLMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.recall.ReCaLLMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.recall.ReCaLLMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.recall.ReCaLLMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.recall.ReCaLLMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.recall.ReCaLLMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.recall.ReCaLLMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.recall.ReCaLLMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.recall.ReCaLLMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.recall.ReCaLLMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput, prefix_token_length: int) -&gt; float\n</code></pre> <p>Process model output and calculate loss (negative log-likelihood)</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Loss</p> </li> </ul>"},{"location":"api-reference/#methods.recall.ReCaLLMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.recall.ReCaLLMethod.process_output(prefix_token_length)","title":"<code>prefix_token_length</code>","text":"(<code>int</code>)           \u2013            <p>Number of prefix tokens to exclude</p>"},{"location":"api-reference/#methods.recall.ReCaLLMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    labels: list[int],\n    model: LLM,\n    tokenizer: AnyTokenizer,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>ReCaLL algorithm to calculate scores for a list of texts Args:     texts: List of texts     labels: List of labels     model: LLM model     tokenizer: Tokenizer     sampling_params: Sampling parameters     lora_request: LoRA request     data_config: Data configuration</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of ReCaLL scores</p> </li> </ul>"},{"location":"api-reference/#methods.recall.ReCaLLMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.recall.ReCaLLMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.samia","title":"samia","text":"<p>Classes:</p> <ul> <li> <code>SaMIAMethod</code>           \u2013            <p>SaMIA membership inference method</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>get_suffix</code>             \u2013              <p>Extracts a suffix from the given text, based on the specified prefix ratio and text length.</p> </li> <li> <code>ngrams</code>             \u2013              <p>Generates n-grams from a sequence.</p> </li> <li> <code>rouge_n</code>             \u2013              <p>Calculates the ROUGE-N score between a candidate and a reference.</p> </li> </ul>"},{"location":"api-reference/#methods.samia.SaMIAMethod","title":"SaMIAMethod","text":"<pre><code>SaMIAMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.samia.SaMIAMethod[SaMIAMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.samia.SaMIAMethod\n                \n\n\n              click methods.samia.SaMIAMethod href \"\" \"methods.samia.SaMIAMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>SaMIA membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Calculate SaMIA score from a single model output</p> </li> <li> <code>run</code>             \u2013              <p>SaMIA algorithm to calculate scores for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.samia.SaMIAMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.samia.SaMIAMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.samia.SaMIAMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.samia.SaMIAMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.samia.SaMIAMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.samia.SaMIAMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.samia.SaMIAMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.samia.SaMIAMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.samia.SaMIAMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.samia.SaMIAMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput) -&gt; float\n</code></pre> <p>Calculate SaMIA score from a single model output Note: This method is called from BaseMethod.run, but for SaMIA, a custom implementation using multiple samples is used, so this method is not supported for single output. Use run method instead.</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>SaMIA score</p> </li> </ul>"},{"location":"api-reference/#methods.samia.SaMIAMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.samia.SaMIAMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>SaMIA algorithm to calculate scores for a list of texts Args:     texts: List of texts     model: LLM model     lora_request: LoRA request     data_config: Data configuration</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of SaMIA scores</p> </li> </ul>"},{"location":"api-reference/#methods.samia.SaMIAMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.samia.SaMIAMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"api-reference/#methods.samia.get_suffix","title":"get_suffix","text":"<pre><code>get_suffix(text: str, prefix_ratio: float, text_length: int) -&gt; list\n</code></pre> <p>Extracts a suffix from the given text, based on the specified prefix ratio and text length.</p>"},{"location":"api-reference/#methods.samia.ngrams","title":"ngrams","text":"<pre><code>ngrams(sequence: str, n: int) -&gt; zip\n</code></pre> <p>Generates n-grams from a sequence.</p>"},{"location":"api-reference/#methods.samia.rouge_n","title":"rouge_n","text":"<pre><code>rouge_n(candidate: list, reference: list, n: int = 1) -&gt; float\n</code></pre> <p>Calculates the ROUGE-N score between a candidate and a reference.</p>"},{"location":"api-reference/#methods.zlib","title":"zlib","text":"<p>Classes:</p> <ul> <li> <code>ZlibMethod</code>           \u2013            <p>Zlib compression-based membership inference method</p> </li> </ul>"},{"location":"api-reference/#methods.zlib.ZlibMethod","title":"ZlibMethod","text":"<pre><code>ZlibMethod(method_config: dict[str, Any] = None)\n</code></pre> <pre><code>\n              flowchart TD\n              methods.zlib.ZlibMethod[ZlibMethod]\n              methods.base.BaseMethod[BaseMethod]\n\n                              methods.base.BaseMethod --&gt; methods.zlib.ZlibMethod\n                \n\n\n              click methods.zlib.ZlibMethod href \"\" \"methods.zlib.ZlibMethod\"\n              click methods.base.BaseMethod href \"\" \"methods.base.BaseMethod\"\n            </code></pre> <p>Zlib compression-based membership inference method</p> <p>Parameters:</p> <p>Methods:</p> <ul> <li> <code>clear_cache</code>             \u2013              <p>Clear cache and reset statistics</p> </li> <li> <code>get_cache_stats</code>             \u2013              <p>Get cache statistics</p> </li> <li> <code>get_outputs</code>             \u2013              <p>Get model outputs for a list of texts</p> </li> <li> <code>process_output</code>             \u2013              <p>Process model output and calculate zlib-compressed information content ratio</p> </li> <li> <code>run</code>             \u2013              <p>Run inference for a list of texts</p> </li> <li> <code>set_max_cache_size</code>             \u2013              <p>Set maximum cache size</p> </li> </ul>"},{"location":"api-reference/#methods.zlib.ZlibMethod(method_config)","title":"<code>method_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Method configuration</p>"},{"location":"api-reference/#methods.zlib.ZlibMethod.clear_cache","title":"clear_cache  <code>classmethod</code>","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Clear cache and reset statistics</p>"},{"location":"api-reference/#methods.zlib.ZlibMethod.get_cache_stats","title":"get_cache_stats  <code>classmethod</code>","text":"<pre><code>get_cache_stats() -&gt; dict[str, Any]\n</code></pre> <p>Get cache statistics</p> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Cache statistics</p> </li> </ul>"},{"location":"api-reference/#methods.zlib.ZlibMethod.get_outputs","title":"get_outputs","text":"<pre><code>get_outputs(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[RequestOutput]\n</code></pre> <p>Get model outputs for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[RequestOutput]</code>           \u2013            <p>List of model outputs</p> </li> </ul>"},{"location":"api-reference/#methods.zlib.ZlibMethod.get_outputs(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.zlib.ZlibMethod.get_outputs(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.zlib.ZlibMethod.get_outputs(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.zlib.ZlibMethod.get_outputs(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.zlib.ZlibMethod.get_outputs(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.zlib.ZlibMethod.process_output","title":"process_output","text":"<pre><code>process_output(output: RequestOutput) -&gt; float\n</code></pre> <p>Process model output and calculate zlib-compressed information content ratio</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>zlib-compressed information content ratio</p> </li> </ul>"},{"location":"api-reference/#methods.zlib.ZlibMethod.process_output(output)","title":"<code>output</code>","text":"(<code>RequestOutput</code>)           \u2013            <p>Model output</p>"},{"location":"api-reference/#methods.zlib.ZlibMethod.run","title":"run","text":"<pre><code>run(\n    texts: list[str],\n    model: LLM,\n    sampling_params: SamplingParams,\n    lora_request: LoRARequest = None,\n    data_config: dict[str, Any] = None,\n) -&gt; list[float]\n</code></pre> <p>Run inference for a list of texts</p> <p>Parameters:</p> <p>Returns:</p> <ul> <li> <code>list[float]</code>           \u2013            <p>List of scores</p> </li> </ul>"},{"location":"api-reference/#methods.zlib.ZlibMethod.run(texts)","title":"<code>texts</code>","text":"(<code>list[str]</code>)           \u2013            <p>List of texts</p>"},{"location":"api-reference/#methods.zlib.ZlibMethod.run(model)","title":"<code>model</code>","text":"(<code>LLM</code>)           \u2013            <p>LLM model</p>"},{"location":"api-reference/#methods.zlib.ZlibMethod.run(sampling_params)","title":"<code>sampling_params</code>","text":"(<code>SamplingParams</code>)           \u2013            <p>Sampling parameters</p>"},{"location":"api-reference/#methods.zlib.ZlibMethod.run(lora_request)","title":"<code>lora_request</code>","text":"(<code>LoRARequest</code>, default:                   <code>None</code> )           \u2013            <p>LoRA request</p>"},{"location":"api-reference/#methods.zlib.ZlibMethod.run(data_config)","title":"<code>data_config</code>","text":"(<code>dict[str, Any]</code>, default:                   <code>None</code> )           \u2013            <p>Data configuration</p>"},{"location":"api-reference/#methods.zlib.ZlibMethod.set_max_cache_size","title":"set_max_cache_size  <code>classmethod</code>","text":"<pre><code>set_max_cache_size(size: int) -&gt; None\n</code></pre> <p>Set maximum cache size</p> <p>Parameters:</p>"},{"location":"api-reference/#methods.zlib.ZlibMethod.set_max_cache_size(size)","title":"<code>size</code>","text":"(<code>int</code>)           \u2013            <p>Maximum cache size (number of entries)</p>"},{"location":"how-to-use/","title":"How to Use","text":"<p>This guide explains how to run the CLI and how to describe each section of the YAML configuration file.</p>"},{"location":"how-to-use/#run-fast-mia","title":"Run Fast-MIA","text":"<ol> <li>Create a configuration file (refer to <code>config/sample.yaml</code>)</li> <li>Run the following command:</li> </ol> <pre><code>uv run --with 'vllm==0.15.1' python main.py --config config/your_own_configuration.yaml\n</code></pre> <p>Note: When using T4 GPUs (e.g., Google Colab), set the environment variable to avoid attention backend issues:</p> <pre><code>VLLM_ATTENTION_BACKEND=XFORMERS uv run --with 'vllm==0.15.1' python main.py --config config/sample.yaml\n</code></pre> <p></p>"},{"location":"how-to-use/#cli-arguments","title":"CLI Arguments","text":"Flag Required Default Purpose <code>--config</code> \u2705 \u2013 Path to the YAML configuration file. <code>--seed</code> \u274c <code>42</code> Global seed passed to <code>random</code>, <code>torch</code>, <code>numpy</code>, and Python to make evaluations reproducible. <code>--max-cache-size</code> \u274c <code>1000</code> Maximum number of vLLM generations cached across methods; the default is sufficient for most runs. <code>--detailed-report</code> \u274c Off Generate detailed report with metadata, per-sample scores, and visualizations."},{"location":"how-to-use/#configuration-files","title":"Configuration Files","text":"<p>All behavior is driven by a YAML file. Each top-level key toggles a different subsystem:</p> Key Purpose <code>model</code> Required. Parameters are directly forwarded to <code>vllm.LLM</code> (model, dtype, etc.). <code>sampling_parameters</code> Optional. Parameters are directly forwarded to <code>vllm.SamplingParams</code>. <code>prompt_logprobs</code> defaults to <code>0</code>, <code>max_tokens</code> defaults to <code>1</code>, <code>temperature</code> defaults to <code>0.0</code>, and <code>top_p</code> defaults to <code>1.0</code> when omitted. <code>lora</code> Optional. Parameters are directly forwarded to <code>vllm.lora.request.LoRARequest</code>. Omit when no adapter is needed. <code>data</code> Required. Specifies the dataset source, file format, column names, etc. <code>methods</code> Required. Ordered list of evaluation methods. Each entry declares a <code>type</code> and method-specific <code>params</code>. <code>output_dir</code> Optional. Directory where evaluation CSVs are stored (<code>./results</code> by default)."},{"location":"how-to-use/#example-configuration","title":"Example Configuration","text":"<p>Please refer to <code>config/sample.yaml</code> for a complete example configuration file.</p>"},{"location":"how-to-use/#model-block","title":"<code>model</code> Block","text":"Field Required Notes <code>model_id</code> \u2705 The name or path of a HuggingFace Transformers model that <code>vllm.LLM</code> can load. (= model) Other keys \u274c Forwarded directly to <code>vllm.LLM</code>. Select params to fit the model onto your hardware, following the vLLM.LLM API Reference."},{"location":"how-to-use/#sampling_parameters-block","title":"<code>sampling_parameters</code> Block","text":"<p>Values are passed to <code>vllm.SamplingParams</code>. Select params following the vLLM.SamplingParams API Reference. Fast-MIA automatically enforces <code>prompt_logprobs: 0</code>, <code>max_tokens: 1</code>, <code>temperature: 0.0</code>, and <code>top_p: 1.0</code> whenever the config omits those keys to ensure deterministic, efficient scoring, but you can override any of these defaults by explicitly setting them inside this block.</p> <p>Recommended defaults for deterministic scoring</p> Field Purpose <code>max_tokens</code> Use <code>1</code> to request only the immediate next token; Fast-MIA defaults to this. <code>prompt_logprobs</code> Use <code>0</code> to get prompt text log-probabilities; Fast-MIA defaults to this. <code>temperature</code> Set to <code>0.0</code> for deterministic runs; Fast-MIA defaults to this. <code>top_p</code> Leave at <code>1.0</code> so determinism is governed solely by <code>temperature</code>; Fast-MIA defaults to this."},{"location":"how-to-use/#lora-block-optional","title":"<code>lora</code> Block (Optional)","text":"<p>Values are passed to <code>vllm.lora.request.LoRARequest</code>. Select params following the vLLM.lora.request.LoRARequest API Reference.</p> Field Purpose <code>lora_name</code> Human-readable adapter name (used in logs). <code>lora_int_id</code> Integer identifier that vLLM uses to cache the adapter. Use different IDs for simultaneous adapters. <code>lora_path</code> Filesystem path to the adapter weights. <p>Omit the entire block if you evaluate the base model. When enabled, the adapter is transparently applied to all methods.</p> <p>Note: If you enable LoRA, there is a known prompt-logprob bug (https://discuss.vllm.ai/t/bug-wrong-lora-mapping-during-prompt-logprobs-computing/500/2). Setting <code>sampling_parameters.prompt_logprobs</code> currently raises an error, so methods like <code>loss</code> or <code>mink</code> cannot run.</p>"},{"location":"how-to-use/#data-block","title":"<code>data</code> Block","text":"Field Required Default Description <code>data_path</code> \u2705 \u2013 Path to a local file or one of the dedicated Hugging Face datasets (<code>swj0419/WikiMIA</code> or <code>iamgroot42/mimir_{domain}_{ngram}</code>). <code>format</code> \u274c <code>csv</code> One of <code>csv</code>, <code>jsonl</code>, <code>json</code>, <code>parquet</code>. Use <code>huggingface</code> only in combination with <code>swj0419/WikiMIA</code> or <code>iamgroot42/mimir_{domain}_{ngram}</code>. <code>text_column</code> \u274c <code>text</code> Column containing the raw text to probe. <code>label_column</code> \u274c <code>label</code> Column containing membership labels (<code>1</code> = member, <code>0</code> = non-member). <code>text_length</code> \u274c <code>32</code> Number of words kept from each sample. WikiMIA requires one of <code>32</code>, <code>64</code>, <code>128</code>, <code>256</code>; MIMIR requires <code>200</code>. <code>space_delimited_language</code> \u274c <code>true</code> Set to <code>false</code> for languages without whitespace like Japanese. <p>Note: If <code>space_delimited_language</code> is <code>false</code>, you must pre-process your text and insert spaces between words beforehand; Fast-MIA assumes the input is already space-separated and will simply strip the spaces back out during scoring.</p>"},{"location":"how-to-use/#supported-file-formats","title":"Supported File Formats","text":"Format Reader <code>csv</code> <code>pandas.read_csv(data_path)</code> <code>jsonl</code> <code>pandas.read_json(data_path, lines=True)</code> <code>json</code> <code>pandas.read_json(data_path)</code> <code>parquet</code> <code>pandas.read_parquet(data_path)</code> <code>huggingface</code> Not available for arbitrary datasets. Use only with the dedicated WikiMIA/MIMIR loaders described below."},{"location":"how-to-use/#supported-hugging-face-datasets","title":"Supported Hugging Face Datasets","text":"<p>Currently, only the following datasets are supported via the <code>huggingface</code> format.</p> <ul> <li>The WikiMIA dataset is handled specially. If you set <code>data_path</code> to \"swj0419/WikiMIA\", it will be automatically recognized. For this dataset, the data corresponding to the specified text length (32, 64, 128, or 256) will be automatically loaded (e.g., \"WikiMIA_length64\").</li> <li>The MIMIR dataset is handled specially too. If you set <code>data_path</code> to \"iamgroot42/mimir_{domain}_{ngram}\", it will be automatically recognized. For this dataset, the data corresponding to the specified domain and ngram will be automatically loaded (e.g., \"iamgroot42/mimir\", \"pile_cc\", \"ngram_7_0.2\").</li> </ul> <p>Note: To use the MIMIR dataset, you need to create a <code>.env</code> file in the project root directory with your Hugging Face token. Create a <code>.env</code> file with the following content:</p> <pre><code>HUGGINGFACE_TOKEN=your_huggingface_token_here\n</code></pre> <p>Replace <code>your_huggingface_token_here</code> with your actual Hugging Face token. You can obtain a token from Hugging Face Settings.</p>"},{"location":"how-to-use/#methods-block","title":"<code>methods</code> Block","text":"<p>Declare any number of methods. Each entry looks like:</p> <pre><code>- type: \"method_name\"\n  params:\n    # method-specific keys\n</code></pre> <p>Available method types and their parameters:</p> Method Name Description Parameters <code>loss</code> Uses the model's loss \u2013 <code>zlib</code> Uses the ratio of information content calculated by Zlib compression \u2013 <code>lower</code> Uses the ratio of loss after lowercasing the text \u2013 <code>mink</code> https://github.com/swj0419/detect-pretrain-code <code>ratio</code> (<code>0.0\u20131.0</code>, default <code>0.5</code>). <code>pac</code> https://github.com/yyy01/PAC <code>alpha</code> (augmentation strength, default <code>0.3</code>), <code>N</code> (augmentations per sample, default <code>5</code>). <code>recall</code> https://github.com/ruoyuxie/recall <code>num_shots</code> (number of prefix texts, default <code>10</code>), <code>pass_window</code> (skip max-length trimming, default <code>False</code>). <code>conrecall</code> https://github.com/WangCheng0116/CON-RECALL Same as <code>recall</code> plus <code>gamma</code> (ratio of member prefixes loss, default <code>0.5</code>). <code>samia</code> https://github.com/nlp-titech/samia <code>num_samples</code> (number of samples, default <code>5</code>), <code>prefix_ratio</code> (ratio of prefix, default <code>0.5</code>), <code>zlib</code> (Use Zlib, default <code>True</code>). <code>dcpdd</code> https://github.com/zhang-wei-chao/DC-PDD <code>file_num</code> (number of C4 text files, default <code>15</code>), <code>max_token_length</code> (max token length, default <code>1024</code>), <code>alpha</code> (upper bound of score, default <code>0.01</code>)."},{"location":"how-to-use/#output_dir","title":"<code>output_dir</code>","text":"<p>Directory where CSV results are written. The folder is created if missing.</p>"},{"location":"how-to-use/#output-files","title":"Output Files","text":""},{"location":"how-to-use/#default-output","title":"Default Output","text":"<p>By default, Fast-MIA saves the following files in a timestamped folder:</p> <pre><code>results/\n\u2514\u2500\u2500 YYYYMMDD-HHMMSS/\n    \u251c\u2500\u2500 config.yaml    # Copy of the configuration used\n    \u251c\u2500\u2500 results.csv    # Summary metrics (AUROC, FPR@95, TPR@5)\n    \u2514\u2500\u2500 report.txt     # Human-readable summary report\n</code></pre>"},{"location":"how-to-use/#detailed-report-mode","title":"Detailed Report Mode","text":"<p>When you run with <code>--detailed-report</code>, Fast-MIA generates additional outputs for benchmarking and analysis:</p> <pre><code>uv run --with 'vllm==0.15.1' python main.py --config config/sample.yaml --detailed-report\n</code></pre>"},{"location":"how-to-use/#output-structure","title":"Output Structure","text":"<pre><code>results/\n\u2514\u2500\u2500 YYYYMMDD-HHMMSS/           # Timestamped folder for each run\n    \u251c\u2500\u2500 config.yaml            # Copy of the configuration used\n    \u251c\u2500\u2500 results.csv            # Summary metrics (AUROC, FPR@95, TPR@5)\n    \u251c\u2500\u2500 detailed_scores.csv    # Per-sample scores for each method\n    \u251c\u2500\u2500 metadata.json          # Execution metadata (JSON format)\n    \u251c\u2500\u2500 metadata.yaml          # Execution metadata (YAML format)\n    \u251c\u2500\u2500 report.txt             # Human-readable summary report\n    \u2514\u2500\u2500 figures/\n        \u251c\u2500\u2500 roc_curves.png         # ROC curves for all methods\n        \u251c\u2500\u2500 score_distributions.png # Score histograms (member vs non-member)\n        \u2514\u2500\u2500 metrics_comparison.png  # Bar chart comparing metrics\n</code></pre>"},{"location":"how-to-use/#metadata-contents","title":"Metadata Contents","text":"<p>The metadata files (<code>metadata.json</code> / <code>metadata.yaml</code>) include:</p> Section Contents <code>experiment</code> Start/end time, duration <code>environment</code> Python version, platform, hostname <code>git</code> Commit hash, branch, dirty status <code>model</code> Model ID, parameters <code>data</code> Dataset path, format, sample counts <code>sampling_parameters</code> vLLM sampling configuration <code>methods</code> List of evaluated methods with parameters <code>cache</code> Cache hit/miss statistics"},{"location":"how-to-use/#detailed-scores","title":"Detailed Scores","text":"<p>The <code>detailed_scores.csv</code> file contains per-sample scores:</p> <pre><code>label,loss,zlib,mink_0.2,recall\n1,0.832,0.654,0.721,0.891\n0,0.234,0.312,0.287,0.156\n...\n</code></pre> <p>This allows for post-hoc analysis, custom visualizations, or statistical testing.</p>"}]}